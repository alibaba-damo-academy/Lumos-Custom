import os
import json
import multiprocessing
from tqdm import tqdm 

from src._predictor import VideoLightAndShadowPredictor

# --- Configuration Section ---
# Define all paths here to make the script easy to configure.

# Directory containing the videos generated by your model.
# We assume video files are named like '0.mp4', '1.mp4', corresponding to their prompt index.
VIDEO_RESULTS_DIR = "results_gen_videos"

# The original prompt file used to generate the videos. This serves as our "ground truth".
# It must be the exact file that `gen_prompts.py` created.
PROMPT_FILE_PATH = "jsonls/final_dataset_2k.jsonl" 

# An intermediate file to store the analysis results from the predictor.
# This is useful for debugging and allows re-running the scoring without re-running inference.
ANALYSIS_RESULTS_FILE = "evaluation_results.jsonl"

# Path to the model weights directory.
WEIGHT_PATH = "weights"

# Device to run the model on.
DEVICE = 'cuda'


def run_inference_stage(predictor, prompts_data):
    """
    Stage 1: Analyze all generated videos and save the results.

    This function iterates through the prompts, finds the corresponding generated video,
    runs the analysis using the predictor, and saves both the original prompt and the
    analysis result to an intermediate file.
    """
    print("--- Starting Stage 1: Inference ---")
    
    analysis_results = []
    
    # Use tqdm for a user-friendly progress bar.
    for idx, prompt_data in enumerate(tqdm(prompts_data, desc="Analyzing Videos")):
        # Construct the expected video filename based on its index in the prompt file.
        video_filename = f"{idx}.mp4"
        video_path = os.path.join(VIDEO_RESULTS_DIR, video_filename)

        if not os.path.exists(video_path):
            print(f"Warning: Video file not found for prompt {idx}: {video_path}. Skipping.")
            continue

        # Run inference to get the analysis of the generated video.
        analysis_result = predictor.inference(video_path)

        if analysis_result:
            # Store both the original prompt and the analysis for later comparison.
            record = {
                "prompt_index": idx,
                "prompt_data": prompt_data,
                "analysis_result": analysis_result
            }
            analysis_results.append(record)

    # Save all analysis results to the intermediate JSONL file.
    with open(ANALYSIS_RESULTS_FILE, 'w', encoding='utf-8') as f:
        for record in analysis_results:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')
            
    print(f"\nInference complete. {len(analysis_results)} videos analyzed and results saved to '{ANALYSIS_RESULTS_FILE}'.")
    return analysis_results


def calculate_scores_stage(analysis_results):
    """
    Stage 2: Compare analysis results with ground truth prompts and calculate scores.

    This function reads the intermediate results, compares the "intended" lighting
    from the prompt with the "analyzed" lighting, and computes a score for each
    of the six lighting dimensions.
    """
    print("\n--- Starting Stage 2: Scoring ---")

    # Define the six categories we want to score.
    categories_to_score = [
        "Direction of Light",
        "Light Source Type",
        "Light Intensity",
        "Color Temperature",
        "Light Changes in Time",
        "Optical Phenomena"
    ]

    # Initialize counters for each category.
    score_counters = {category: {"correct": 0, "total": 0} for category in categories_to_score}

    for record in tqdm(analysis_results, desc="Calculating Scores"):
        # The "intended" lighting attributes from the original prompt.
        intended_config = record.get("prompt_data", {}).get("lighting_config", {})
        
        # The "analyzed" lighting attributes from the model's prediction.
        analyzed_config = record.get("analysis_result", {}).get("qualitative", {})

        if not intended_config or not analyzed_config:
            # Skip if the necessary data is missing for this record.
            continue
            
        # Compare each category.
        for category in categories_to_score:
            intended_value = intended_config.get(category)
            analyzed_value = analyzed_config.get(category)
            
            if intended_value is not None and analyzed_value is not None:
                # Increment the total count for this category.
                score_counters[category]["total"] += 1
                
                # Check if the analyzed value matches the intended value.
                if str(intended_value).strip() == str(analyzed_value).strip():
                    score_counters[category]["correct"] += 1

    # --- Calculate and Print Final Report ---
    final_scores = {}
    all_scores = []

    print("\n--- LumosBench Evaluation Report ---")
    print("-" * 40)
    for category, counts in score_counters.items():
        total = counts["total"]
        correct = counts["correct"]
        
        # Calculate the score (0-1), handling division by zero.
        score = (correct / total) if total > 0 else 0.0
        final_scores[category] = score
        all_scores.append(score)
        
        print(f"{category:<30}: {score:.3f} ({correct}/{total})")
    
    print("-" * 40)
    
    # Calculate the final average score.
    average_score = sum(all_scores) / len(all_scores) if all_scores else 0.0
    final_scores['Average_Score'] = average_score
    
    print(f"{'Average Score':<30}: {average_score:.3f}")
    print("-" * 40)
    
    return final_scores


if __name__ == '__main__':
    # Set the multiprocessing start method to 'spawn'.
    # This is crucial for libraries like PyTorch/vLLM that use CUDA in subprocesses.
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        # This might already be set, which is fine.
        pass

    # --- Step 1: Initialize the Predictor ---
    print("Initializing VideoLightAndShadowPredictor...")
    predictor = VideoLightAndShadowPredictor(WEIGHT_PATH, DEVICE)
    print("Predictor initialized.")

    # --- Step 2: Load the Ground Truth Prompts ---
    try:
        with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:
            prompts = [json.loads(line) for line in f]
        print(f"Loaded {len(prompts)} prompts from '{PROMPT_FILE_PATH}'.")
    except FileNotFoundError:
        print(f"Error: Prompt file not found at '{PROMPT_FILE_PATH}'. Please run 'gen_prompts.py' first.")
        exit()

    # --- Step 3: Run the Inference Stage ---
    # This will analyze all videos and save results.
    results = run_inference_stage(predictor, prompts)

    # --- Step 4: Run the Scoring Stage ---
    # This will compare results and print the final report.
    if results:
        calculate_scores_stage(results)
    else:
        print("No analysis results were generated. Scoring cannot proceed.")

